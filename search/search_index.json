{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Bodhi LSF \u2192 SLURM Migration Guide","text":"<p>Questions?</p> <p>Contact David Farrell with any questions about the Bodhi LSF-to-SLURM migration.</p> <p>The Bodhi HPC cluster is migrating from IBM Spectrum LSF to SLURM. This guide helps you convert your existing LSF job scripts and workflows to SLURM.</p>"},{"location":"#quick-start-checklist","title":"Quick-start checklist","text":"<ul> <li>\ud83d\udd00 Replace <code>#BSUB</code> directives with <code>#SBATCH</code> equivalents (Directives)</li> <li>\ud83d\udda5\ufe0f Update submission and monitoring commands (Commands)</li> <li>\ud83d\udd24 Replace <code>$LSB_*</code> environment variables with <code>$SLURM_*</code> equivalents (Environment Variables)</li> <li>\ud83d\udd22 Review job array syntax changes (Job Arrays)</li> <li>\u2705 Test your converted scripts with a short run before submitting production jobs</li> </ul>"},{"location":"#key-differences-at-a-glance","title":"Key differences at a glance","text":"Concept LSF SLURM Scheduler directive <code>#BSUB</code> <code>#SBATCH</code> Submit a job <code>bsub &lt; script.sh</code> <code>sbatch script.sh</code> Job status <code>bjobs</code> <code>squeue</code> Cancel a job <code>bkill</code> <code>scancel</code> Interactive session <code>bsub -Is -q interactive bash</code> <code>srun --pty bash</code> Array index variable <code>$LSB_JOBINDEX</code> <code>$SLURM_ARRAY_TASK_ID</code> <p>Use the converter script</p> <p>We provide a sed-based helper script that handles the most common directive and variable substitutions automatically. It's a great starting point \u2014 just review the output before submitting.</p>"},{"location":"#what-stays-the-same","title":"What stays the same","text":"<ul> <li>Shell scripts are still shell scripts. Only the scheduler directives and environment variables change; your actual commands (<code>samtools</code>, <code>R</code>, <code>python</code>, etc.) remain the same.</li> <li>Stdout/stderr are still captured to files \u2014 the default file naming just differs slightly.</li> <li>Module system (<code>module load ...</code>) is unchanged.</li> </ul>"},{"location":"#where-to-get-help","title":"Where to get help","text":"<ul> <li>Browse this guide using the sidebar navigation</li> <li>Check the Resources page for links to official SLURM documentation</li> <li>Contact the Bodhi HPC support team with migration questions</li> </ul>"},{"location":"commands/","title":"Commands: LSF \u2192 SLURM","text":"<p>This page maps common LSF commands to their SLURM equivalents.</p>"},{"location":"commands/#command-mapping-table","title":"Command mapping table","text":"Action LSF SLURM Notes Submit a batch job <code>bsub &lt; script.sh</code> <code>sbatch script.sh</code> SLURM does not need input redirection Interactive session <code>bsub -Is -q interactive bash</code> <code>srun --pty bash</code> Add resource flags as needed View job queue <code>bjobs</code> <code>squeue -u $USER</code> View all jobs <code>bjobs -a</code> <code>squeue</code> Detailed job info <code>bjobs -l &lt;jobid&gt;</code> <code>scontrol show job &lt;jobid&gt;</code> Cancel a job <code>bkill &lt;jobid&gt;</code> <code>scancel &lt;jobid&gt;</code> Cancel all my jobs <code>bkill 0</code> <code>scancel -u $USER</code> Hold a job <code>bstop &lt;jobid&gt;</code> <code>scontrol hold &lt;jobid&gt;</code> Release a held job <code>bresume &lt;jobid&gt;</code> <code>scontrol release &lt;jobid&gt;</code> View cluster partitions <code>bqueues</code> <code>sinfo</code> View partition details <code>bqueues -l &lt;queue&gt;</code> <code>sinfo -p &lt;partition&gt;</code> Job history / accounting <code>bhist &lt;jobid&gt;</code> <code>sacct -j &lt;jobid&gt;</code> Past job efficiency \u2014 <code>seff &lt;jobid&gt;</code> SLURM-only; shows CPU/memory efficiency View cluster load <code>lsload</code> <code>sinfo -N -l</code> Node status <code>bhosts</code> <code>sinfo -N</code> View job dependencies <code>bjobs -l &lt;jobid&gt;</code> <code>scontrol show job &lt;jobid&gt;</code> Check <code>Dependency</code> field Peek at job output <code>bpeek &lt;jobid&gt;</code> Read the <code>--output</code> file directly No built-in equivalent Modify pending job <code>bmod</code> <code>scontrol update job &lt;jobid&gt;</code>"},{"location":"commands/#submitting-jobs","title":"Submitting jobs","text":"LSFSLURM <pre><code># Submit a script\nbsub &lt; myjob.sh\n\n# Submit with inline options\nbsub -q normal -n 4 -W 2:00 -o out.%J &lt; myjob.sh\n\n# Submit with command directly\nbsub -q short -o out.%J \"echo hello\"\n</code></pre> <pre><code># Submit a script\nsbatch myjob.sh\n\n# Submit with inline options\nsbatch --partition=normal --ntasks=4 --time=02:00:00 --output=out.%j myjob.sh\n\n# Submit with command directly (use --wrap)\nsbatch --partition=short --output=out.%j --wrap=\"echo hello\"\n</code></pre> <p>No input redirection needed</p> <p>In LSF you pipe the script into <code>bsub</code> with <code>&lt;</code>. In SLURM you pass the script filename as an argument to <code>sbatch</code>. If you forget and use <code>sbatch &lt; script.sh</code>, it will still work \u2014 but the standard form is <code>sbatch script.sh</code>.</p>"},{"location":"commands/#monitoring-jobs","title":"Monitoring jobs","text":"LSFSLURM <pre><code># My running/pending jobs\nbjobs\n\n# Detailed info on a specific job\nbjobs -l 12345\n\n# All jobs in a queue\nbjobs -q normal\n</code></pre> <pre><code># My running/pending jobs\nsqueue -u $USER\n\n# Detailed info on a specific job\nscontrol show job 12345\n\n# All jobs in a partition\nsqueue -p normal\n</code></pre>"},{"location":"commands/#job-accounting","title":"Job accounting","text":"<p>After a job completes, use <code>sacct</code> to review its resource usage:</p> <pre><code># Basic accounting for a completed job\nsacct -j 12345 --format=JobID,JobName,Partition,Elapsed,MaxRSS,State\n\n# Quick efficiency summary\nseff 12345\n</code></pre> <p><code>seff</code> is your friend</p> <p><code>seff &lt;jobid&gt;</code> gives a quick summary of CPU and memory efficiency for completed jobs. Use it to right-size your future resource requests.</p>"},{"location":"conversion-script/","title":"Converter Script: <code>lsf2slurm.sh</code>","text":"<p>We provide a helper script that automates the most common LSF-to-SLURM conversions. It's located at <code>scripts/lsf2slurm.sh</code> in this repository.</p>"},{"location":"conversion-script/#usage","title":"Usage","text":"<pre><code># Convert a script (prints to stdout)\nbash scripts/lsf2slurm.sh myjob.lsf\n\n# Save the output to a new file\nbash scripts/lsf2slurm.sh myjob.lsf &gt; myjob.slurm\n\n# Or redirect in place (use with caution)\nbash scripts/lsf2slurm.sh myjob.sh &gt; myjob_slurm.sh\n</code></pre>"},{"location":"conversion-script/#what-it-converts","title":"What it converts","text":"<p>The script handles these substitutions:</p>"},{"location":"conversion-script/#directives","title":"Directives","text":"LSF SLURM <code>#BSUB -J name</code> <code>#SBATCH --job-name=name</code> <code>#BSUB -q queue</code> <code>#SBATCH --partition=queue</code> <code>#BSUB -W HH:MM</code> <code>#SBATCH --time=HH:MM:00</code> <code>#BSUB -n N</code> <code>#SBATCH --ntasks=N</code> <code>#BSUB -M mem</code> <code>#SBATCH --mem=memMB</code> <code>#BSUB -o file</code> <code>#SBATCH --output=file</code> <code>#BSUB -e file</code> <code>#SBATCH --error=file</code> <code>#BSUB -N</code> <code>#SBATCH --mail-type=END</code> <code>#BSUB -B</code> <code>#SBATCH --mail-type=BEGIN</code> <code>#BSUB -u email</code> <code>#SBATCH --mail-user=email</code> <code>#BSUB -P project</code> <code>#SBATCH --account=project</code>"},{"location":"conversion-script/#output-filename-tokens","title":"Output filename tokens","text":"LSF SLURM <code>%J</code> <code>%j</code> <code>%I</code> <code>%a</code>"},{"location":"conversion-script/#environment-variables","title":"Environment variables","text":"LSF SLURM <code>$LSB_JOBID</code> <code>$SLURM_JOB_ID</code> <code>$LSB_JOBINDEX</code> <code>$SLURM_ARRAY_TASK_ID</code> <code>$LSB_JOBNAME</code> <code>$SLURM_JOB_NAME</code> <code>$LSB_QUEUE</code> <code>$SLURM_JOB_PARTITION</code> <code>$LSB_SUBCWD</code> <code>$SLURM_SUBMIT_DIR</code> <code>$LSB_DJOB_NUMPROC</code> <code>$SLURM_NTASKS</code> <code>$LSB_HOSTS</code> <code>$SLURM_JOB_NODELIST</code>"},{"location":"conversion-script/#array-syntax","title":"Array syntax","text":"<p>Job array specifications in <code>-J \"name[1-100]\"</code> are converted to separate <code>--job-name=name</code> and <code>--array=1-100</code> directives.</p>"},{"location":"conversion-script/#what-it-does-not-convert","title":"What it does NOT convert","text":"<p>Manual review required</p> <p>The converter is a starting point, not a complete solution. You must review the output before submitting. The following require manual conversion:</p> <ul> <li>Complex <code>-R</code> resource strings \u2014 e.g., <code>rusage[mem=X]</code>, <code>span[hosts=1]</code>, GPU resource requests. These need to be translated to <code>--mem-per-cpu</code>, <code>--nodes</code>, <code>--gpus</code>, etc. based on your specific needs.</li> <li><code>bsub</code> command-line invocations \u2014 only <code>#BSUB</code> directives inside scripts are converted, not <code>bsub</code> commands in wrapper scripts or pipelines.</li> <li>Dependency expressions \u2014 <code>-w \"done(123) &amp;&amp; done(456)\"</code> requires manual translation to <code>--dependency=afterok:123:456</code>.</li> <li>Time format differences \u2014 LSF accepts <code>-W 60</code> (minutes) or <code>-W 1:00</code> (H:MM). The script converts <code>H:MM</code> to <code>HH:MM:00</code> but does not convert bare minute values.</li> <li>Memory unit conversion \u2014 LSF <code>-M</code> values are passed through as MB. If your cluster used KB for <code>-M</code>, you'll need to adjust.</li> <li>Conditional logic using LSF variables \u2014 if your script has complex logic around <code>$LSB_*</code> variables beyond simple references, review carefully.</li> </ul>"},{"location":"conversion-script/#example","title":"Example","text":"<p>Input (<code>myjob.lsf</code>):</p> <pre><code>#!/bin/bash\n#BSUB -J \"analysis[1-50]%10\"\n#BSUB -q normal\n#BSUB -W 4:00\n#BSUB -n 8\n#BSUB -M 16000\n#BSUB -o logs/analysis.%J.%I.out\n#BSUB -e logs/analysis.%J.%I.err\n\nSAMPLE=$(sed -n \"${LSB_JOBINDEX}p\" samples.txt)\n./process.sh $SAMPLE $LSB_DJOB_NUMPROC\n</code></pre> <p>Output:</p> <pre><code>#!/bin/bash\n#SBATCH --job-name=analysis\n#SBATCH --array=1-50%10\n#SBATCH --partition=normal\n#SBATCH --time=04:00:00\n#SBATCH --ntasks=8\n#SBATCH --mem=16000MB\n#SBATCH --output=logs/analysis.%j.%a.out\n#SBATCH --error=logs/analysis.%j.%a.err\n\nSAMPLE=$(sed -n \"${SLURM_ARRAY_TASK_ID}p\" samples.txt)\n./process.sh $SAMPLE $SLURM_NTASKS\n</code></pre>"},{"location":"directives/","title":"Directives: <code>#BSUB</code> \u2192 <code>#SBATCH</code>","text":"<p>Every <code>#BSUB</code> directive in your LSF scripts has a SLURM equivalent using <code>#SBATCH</code>. The table below covers the most common mappings.</p>"},{"location":"directives/#directive-mapping-table","title":"Directive mapping table","text":"Purpose LSF (<code>#BSUB</code>) SLURM (<code>#SBATCH</code>) Notes Job name <code>-J myJob</code> <code>--job-name=myJob</code> Queue / Partition <code>-q normal</code> <code>--partition=normal</code> LSF \"queues\" are SLURM \"partitions\" Wall-clock time limit <code>-W 60</code> or <code>-W 1:00</code> <code>--time=01:00:00</code> SLURM uses <code>HH:MM:SS</code> or <code>D-HH:MM:SS</code> Stdout file <code>-o output.%J</code> <code>--output=output.%j</code> See filename patterns below Stderr file <code>-e error.%J</code> <code>--error=error.%j</code> Combine stdout/stderr <code>-o output.%J</code> (default) <code>--output=output.%j</code> SLURM merges by default when <code>--error</code> is omitted Memory per job <code>-M 4000</code> <code>--mem=4G</code> LSF uses KB by default; SLURM uses MB (suffix <code>G</code> for GB) Memory per core <code>-R \"rusage[mem=4000]\"</code> <code>--mem-per-cpu=4G</code> Number of cores <code>-n 4</code> <code>--ntasks=4</code> or <code>--cpus-per-task=4</code> Use <code>--cpus-per-task</code> for threaded jobs Number of nodes <code>-R \"span[hosts=1]\"</code> <code>--nodes=1</code> Exclusive node <code>-x</code> <code>--exclusive</code> GPU request <code>-R \"rusage[ngpus_physical=1]\"</code> <code>--gpus=1</code> or <code>--gres=gpu:1</code> Syntax varies by cluster config GPU type <code>-R \"select[ngpus_physical&gt;0] rusage[ngpus_physical=1]\" -q gpu</code> <code>--gpus=a100:1</code> or <code>--gres=gpu:a100:1</code> Job array <code>-J \"myJob[1-100]\"</code> <code>--array=1-100</code> See Job Arrays Array concurrency limit <code>-J \"myJob[1-100]%10\"</code> <code>--array=1-100%10</code> Max simultaneous tasks Dependency (after OK) <code>-w \"done(12345)\"</code> <code>--dependency=afterok:12345</code> Dependency (after any) <code>-w \"ended(12345)\"</code> <code>--dependency=afterany:12345</code> Email address <code>-u user@example.com</code> <code>--mail-user=user@example.com</code> Email on start <code>-B</code> <code>--mail-type=BEGIN</code> Email on end <code>-N</code> <code>--mail-type=END</code> Email on start and end <code>-B -N</code> <code>--mail-type=BEGIN,END</code> Project / Account <code>-P myproject</code> <code>--account=myproject</code> Working directory (submit from dir) <code>--chdir=/path/to/dir</code> LSF runs from submission dir by default; so does SLURM Hold job <code>-H</code> <code>--hold</code> Requeue on failure <code>-r</code> <code>--requeue</code>"},{"location":"directives/#output-filename-patterns","title":"Output filename patterns","text":"<p>LSF and SLURM use different tokens for dynamic filenames:</p> Meaning LSF SLURM Job ID <code>%J</code> <code>%j</code> Array index <code>%I</code> <code>%a</code> Job name <code>%J</code> <code>%x</code> Array Job ID <code>%J</code> <code>%A</code> Node (first) \u2014 <code>%N</code> <p>Case matters</p> <p>LSF uses uppercase <code>%J</code> and <code>%I</code>. SLURM uses lowercase <code>%j</code> and <code>%a</code>. Forgetting to change the case will result in literal <code>%J</code> appearing in your filenames.</p>"},{"location":"directives/#example","title":"Example","text":"LSFSLURM <pre><code>#!/bin/bash\n#BSUB -J alignment\n#BSUB -q normal\n#BSUB -W 4:00\n#BSUB -n 8\n#BSUB -M 16000\n#BSUB -o alignment.%J.out\n#BSUB -e alignment.%J.err\n\nmodule load samtools\nsamtools sort -@ 8 input.bam -o sorted.bam\n</code></pre> <pre><code>#!/bin/bash\n#SBATCH --job-name=alignment\n#SBATCH --partition=normal\n#SBATCH --time=04:00:00\n#SBATCH --cpus-per-task=8\n#SBATCH --mem=16G\n#SBATCH --output=alignment.%j.out\n#SBATCH --error=alignment.%j.err\n\nmodule load samtools\nsamtools sort -@ 8 input.bam -o sorted.bam\n</code></pre> <p>Memory units</p> <p>LSF's <code>-M</code> typically specifies KB. SLURM's <code>--mem</code> defaults to MB, but you can use suffixes: <code>--mem=16G</code> for 16 GB. Always double-check units when converting.</p>"},{"location":"environment-variables/","title":"Environment Variables: <code>$LSB_*</code> \u2192 <code>$SLURM_*</code>","text":"<p>LSF and SLURM both set environment variables inside running jobs. You'll need to update any scripts that reference <code>$LSB_*</code> variables.</p>"},{"location":"environment-variables/#variable-mapping-table","title":"Variable mapping table","text":"Purpose LSF SLURM Job ID <code>$LSB_JOBID</code> <code>$SLURM_JOB_ID</code> (or <code>$SLURM_JOBID</code>) Job name <code>$LSB_JOBNAME</code> <code>$SLURM_JOB_NAME</code> Queue / Partition <code>$LSB_QUEUE</code> <code>$SLURM_JOB_PARTITION</code> Submit directory <code>$LSB_SUBCWD</code> <code>$SLURM_SUBMIT_DIR</code> Hostname list <code>$LSB_HOSTS</code> <code>$SLURM_JOB_NODELIST</code> Number of processors <code>$LSB_DJOB_NUMPROC</code> <code>$SLURM_NTASKS</code> Array job index <code>$LSB_JOBINDEX</code> <code>$SLURM_ARRAY_TASK_ID</code> Array job ID (parent) <code>$LSB_JOBID</code> <code>$SLURM_ARRAY_JOB_ID</code> Max array index <code>$LSB_JOBINDEX_END</code> <code>$SLURM_ARRAY_TASK_MAX</code> Allocated GPUs <code>$LSB_GPU_ALLOC</code> <code>$SLURM_GPUS_ON_NODE</code> Temporary directory <code>$LSB_TMPDIR</code> or <code>$TMPDIR</code> <code>$TMPDIR</code>"},{"location":"environment-variables/#slurm-only-variables-worth-knowing","title":"SLURM-only variables worth knowing","text":"<p>These variables don't have direct LSF equivalents but are useful in SLURM scripts:</p> Variable Description <code>$SLURM_CPUS_PER_TASK</code> Number of CPUs allocated per task (set by <code>--cpus-per-task</code>) <code>$SLURM_MEM_PER_NODE</code> Memory allocated per node in MB <code>$SLURM_ARRAY_TASK_MIN</code> First index in the array <code>$SLURM_ARRAY_TASK_STEP</code> Step size between array indices <code>$SLURM_ARRAY_TASK_COUNT</code> Total number of tasks in the array <code>$SLURM_JOB_NUM_NODES</code> Number of nodes allocated <code>$SLURM_NODELIST</code> List of nodes allocated (compact format) <code>$SLURM_GPUS</code> Total number of GPUs allocated"},{"location":"environment-variables/#example-updating-a-script","title":"Example: updating a script","text":"LSFSLURM <pre><code>#!/bin/bash\n#BSUB -J myjob\n#BSUB -o logs/%J.out\n\necho \"Job $LSB_JOBID running on $LSB_HOSTS\"\necho \"Working from $LSB_SUBCWD\"\necho \"Using $LSB_DJOB_NUMPROC processors\"\n\ncd $LSB_SUBCWD\n./my_program --threads $LSB_DJOB_NUMPROC\n</code></pre> <pre><code>#!/bin/bash\n#SBATCH --job-name=myjob\n#SBATCH --output=logs/%j.out\n\necho \"Job $SLURM_JOB_ID running on $SLURM_JOB_NODELIST\"\necho \"Working from $SLURM_SUBMIT_DIR\"\necho \"Using $SLURM_NTASKS processors\"\n\ncd $SLURM_SUBMIT_DIR\n./my_program --threads $SLURM_NTASKS\n</code></pre> <p>Search and replace</p> <p>A quick way to catch variable references is to search your scripts for <code>LSB_</code>:</p> <pre><code>grep -rn 'LSB_' *.sh\n</code></pre> <p>The converter script handles the most common variable substitutions automatically.</p>"},{"location":"example-scripts/","title":"Example Scripts","text":"<p>Complete before/after examples for common job types. Each example shows the full LSF script alongside its SLURM equivalent.</p>"},{"location":"example-scripts/#simple-job","title":"Simple job","text":"LSFSLURM <pre><code>#!/bin/bash\n#BSUB -J simple_job\n#BSUB -q normal\n#BSUB -W 1:00\n#BSUB -n 1\n#BSUB -M 4000\n#BSUB -o simple.%J.out\n#BSUB -e simple.%J.err\n\nmodule load R/4.3.0\n\nRscript my_analysis.R\n</code></pre> <pre><code>#!/bin/bash\n#SBATCH --job-name=simple_job\n#SBATCH --partition=normal\n#SBATCH --time=01:00:00\n#SBATCH --ntasks=1\n#SBATCH --mem=4G\n#SBATCH --output=simple.%j.out\n#SBATCH --error=simple.%j.err\n\nmodule load R/4.3.0\n\nRscript my_analysis.R\n</code></pre>"},{"location":"example-scripts/#array-job","title":"Array job","text":"LSFSLURM <pre><code>#!/bin/bash\n#BSUB -J \"align[1-50]%10\"\n#BSUB -q normal\n#BSUB -W 4:00\n#BSUB -n 8\n#BSUB -M 16000\n#BSUB -o logs/align.%J.%I.out\n#BSUB -e logs/align.%J.%I.err\n\nmodule load samtools/1.17\nmodule load bwa/0.7.17\n\nSAMPLE=$(sed -n \"${LSB_JOBINDEX}p\" samples.txt)\nR1=fastq/${SAMPLE}_R1.fastq.gz\nR2=fastq/${SAMPLE}_R2.fastq.gz\n\nbwa mem -t 8 ref/genome.fa $R1 $R2 \\\n    | samtools sort -@ 4 -o bam/${SAMPLE}.sorted.bam\n\nsamtools index bam/${SAMPLE}.sorted.bam\n</code></pre> <pre><code>#!/bin/bash\n#SBATCH --job-name=align\n#SBATCH --partition=normal\n#SBATCH --time=04:00:00\n#SBATCH --cpus-per-task=8\n#SBATCH --mem=16G\n#SBATCH --array=1-50%10\n#SBATCH --output=logs/align.%A.%a.out\n#SBATCH --error=logs/align.%A.%a.err\n\nmodule load samtools/1.17\nmodule load bwa/0.7.17\n\nSAMPLE=$(sed -n \"${SLURM_ARRAY_TASK_ID}p\" samples.txt)\nR1=fastq/${SAMPLE}_R1.fastq.gz\nR2=fastq/${SAMPLE}_R2.fastq.gz\n\nbwa mem -t 8 ref/genome.fa $R1 $R2 \\\n    | samtools sort -@ 4 -o bam/${SAMPLE}.sorted.bam\n\nsamtools index bam/${SAMPLE}.sorted.bam\n</code></pre>"},{"location":"example-scripts/#gpu-job","title":"GPU job","text":"LSFSLURM <pre><code>#!/bin/bash\n#BSUB -J gpu_train\n#BSUB -q gpu\n#BSUB -W 24:00\n#BSUB -n 4\n#BSUB -M 32000\n#BSUB -R \"select[ngpus_physical&gt;0] rusage[ngpus_physical=1]\"\n#BSUB -o train.%J.out\n#BSUB -e train.%J.err\n\nmodule load cuda/11.8\nmodule load anaconda3\n\nconda activate ml_env\n\npython train.py \\\n    --epochs 100 \\\n    --batch-size 64 \\\n    --gpus 1\n</code></pre> <pre><code>#!/bin/bash\n#SBATCH --job-name=gpu_train\n#SBATCH --partition=gpu\n#SBATCH --time=24:00:00\n#SBATCH --cpus-per-task=4\n#SBATCH --mem=32G\n#SBATCH --gpus=1\n#SBATCH --output=train.%j.out\n#SBATCH --error=train.%j.err\n\nmodule load cuda/11.8\nmodule load anaconda3\n\nconda activate ml_env\n\npython train.py \\\n    --epochs 100 \\\n    --batch-size 64 \\\n    --gpus 1\n</code></pre>"},{"location":"example-scripts/#job-with-dependencies","title":"Job with dependencies","text":"LSFSLURM <pre><code># Step 1: Alignment\nJOB1=$(bsub -J align -q normal -W 4:00 -n 8 -o align.%J.out &lt; align.sh | grep -oP '\\d+')\n\n# Step 2: Sort (after align completes successfully)\nJOB2=$(bsub -J sort -q normal -W 2:00 -n 4 -w \"done($JOB1)\" -o sort.%J.out &lt; sort.sh | grep -oP '\\d+')\n\n# Step 3: Stats (after sort completes successfully)\nbsub -J stats -q short -W 0:30 -n 1 -w \"done($JOB2)\" -o stats.%J.out &lt; stats.sh\n</code></pre> <pre><code># Step 1: Alignment\nJOB1=$(sbatch --job-name=align --partition=normal --time=04:00:00 --cpus-per-task=8 --output=align.%j.out align.sh | awk '{print $4}')\n\n# Step 2: Sort (after align completes successfully)\nJOB2=$(sbatch --job-name=sort --partition=normal --time=02:00:00 --cpus-per-task=4 --dependency=afterok:$JOB1 --output=sort.%j.out sort.sh | awk '{print $4}')\n\n# Step 3: Stats (after sort completes successfully)\nsbatch --job-name=stats --partition=short --time=00:30:00 --ntasks=1 --dependency=afterok:$JOB2 --output=stats.%j.out stats.sh\n</code></pre> <p>Capturing the job ID</p> <ul> <li>LSF: <code>bsub</code> prints <code>Job &lt;12345&gt; is submitted to queue &lt;normal&gt;.</code> \u2014 parse with <code>grep -oP '\\d+'</code></li> <li>SLURM: <code>sbatch</code> prints <code>Submitted batch job 12345</code> \u2014 parse with <code>awk '{print $4}'</code></li> </ul>"},{"location":"example-scripts/#interactive-session","title":"Interactive session","text":"LSFSLURM <pre><code># Basic interactive session\nbsub -Is -q interactive bash\n\n# With resources\nbsub -Is -q interactive -n 4 -M 8000 -W 2:00 bash\n\n# Interactive with GPU\nbsub -Is -q gpu -R \"rusage[ngpus_physical=1]\" -n 4 -M 16000 bash\n</code></pre> <pre><code># Basic interactive session\nsrun --pty bash\n\n# With resources\nsrun --partition=interactive --cpus-per-task=4 --mem=8G --time=02:00:00 --pty bash\n\n# Interactive with GPU\nsrun --partition=gpu --gpus=1 --cpus-per-task=4 --mem=16G --pty bash\n</code></pre> <p>Interactive jobs</p> <p>In SLURM, <code>srun --pty bash</code> gives you an interactive shell on a compute node. You can also use <code>salloc</code> to allocate resources first, then <code>srun</code> within that allocation.</p>"},{"location":"job-arrays/","title":"Job Arrays","text":"<p>Job arrays are one of the areas where LSF and SLURM differ the most. This page covers the syntax changes, index variable differences, and common pitfalls.</p>"},{"location":"job-arrays/#syntax-comparison","title":"Syntax comparison","text":"Feature LSF SLURM Basic array <code>#BSUB -J \"myJob[1-100]\"</code> <code>#SBATCH --array=1-100</code> With step <code>#BSUB -J \"myJob[1-100:2]\"</code> <code>#SBATCH --array=1-100:2</code> Specific indices <code>#BSUB -J \"myJob[1,3,5,7]\"</code> <code>#SBATCH --array=1,3,5,7</code> Concurrency limit <code>#BSUB -J \"myJob[1-100]%10\"</code> <code>#SBATCH --array=1-100%10</code> Index variable <code>$LSB_JOBINDEX</code> <code>$SLURM_ARRAY_TASK_ID</code> <p>Array index starts</p> <p>Both LSF and SLURM support arbitrary start indices. However, if your LSF arrays start at <code>0</code>, double-check that your code handles index <code>0</code> correctly \u2014 some tools expect 1-based indexing.</p>"},{"location":"job-arrays/#index-variable-change","title":"Index variable change","text":"<p>This is the most critical change. Everywhere your script uses <code>$LSB_JOBINDEX</code>, replace it with <code>$SLURM_ARRAY_TASK_ID</code>.</p> LSFSLURM <pre><code>#!/bin/bash\n#BSUB -J \"process[1-50]\"\n#BSUB -o logs/process.%J.%I.out\n\nSAMPLE=$(sed -n \"${LSB_JOBINDEX}p\" samples.txt)\necho \"Processing sample $SAMPLE (index $LSB_JOBINDEX)\"\n</code></pre> <pre><code>#!/bin/bash\n#SBATCH --job-name=process\n#SBATCH --array=1-50\n#SBATCH --output=logs/process.%A.%a.out\n\nSAMPLE=$(sed -n \"${SLURM_ARRAY_TASK_ID}p\" samples.txt)\necho \"Processing sample $SAMPLE (index $SLURM_ARRAY_TASK_ID)\"\n</code></pre>"},{"location":"job-arrays/#array-specification-options","title":"Array specification options","text":"<p>SLURM's <code>--array</code> flag is flexible:</p> <pre><code># Range\n#SBATCH --array=1-100\n\n# Range with step\n#SBATCH --array=0-20:5          # 0, 5, 10, 15, 20\n\n# Explicit list\n#SBATCH --array=1,4,7,22\n\n# Range with concurrency limit (max 10 running at once)\n#SBATCH --array=1-1000%10\n\n# Combined range and list (SLURM only)\n#SBATCH --array=1-5,10,15-20\n</code></pre>"},{"location":"job-arrays/#output-file-naming","title":"Output file naming","text":"<p>For array jobs, use <code>%A</code> (array job ID) and <code>%a</code> (array task index) in output filenames:</p> <pre><code>#SBATCH --output=logs/%x.%A.%a.out    # jobname.jobid.taskindex.out\n#SBATCH --error=logs/%x.%A.%a.err\n</code></pre> LSF pattern SLURM pattern Expands to <code>%J</code> (in array job) <code>%A</code> Parent array job ID <code>%I</code> <code>%a</code> Array task index"},{"location":"job-arrays/#best-practices","title":"Best practices","text":"<ol> <li> <p>Always set a concurrency limit with <code>%N</code> to avoid overwhelming the scheduler or shared resources:    <pre><code>#SBATCH --array=1-1000%50\n</code></pre></p> </li> <li> <p>Use meaningful output filenames that include both the job ID and array index:    <pre><code>#SBATCH --output=logs/%x_%A_%a.out\n</code></pre></p> </li> <li> <p>Test with a small range first before submitting large arrays:    <pre><code>#SBATCH --array=1-3    # test\n# then change to --array=1-1000%50\n</code></pre></p> </li> </ol>"},{"location":"job-arrays/#common-pitfalls","title":"Common pitfalls","text":"<p>Output file clobbering</p> <p>If you forget to include <code>%a</code> in the output filename, all array tasks will write to the same file and overwrite each other. Always include the array index in output filenames for array jobs.</p> <p>Max array size</p> <p>SLURM clusters have a maximum array size limit (often 1000 or 10000, set by <code>MaxArraySize</code> in <code>slurm.conf</code>). If you need more tasks than the limit, split into multiple submissions or use a wrapper script. Check the limit with:</p> <pre><code>scontrol show config | grep MaxArraySize\n</code></pre> <p>Concurrency limit syntax</p> <p>The <code>%N</code> concurrency limit goes at the end of the <code>--array</code> spec. <code>--array=1-100%10</code> means indices 1\u2013100 with at most 10 running simultaneously. This is identical to LSF's syntax \u2014 one thing that didn't change!</p>"},{"location":"pain-points/","title":"Common Pain Points","text":"<p>This page covers recurring issues that Bodhi users encounter when migrating from LSF to SLURM. These aren't simple directive swaps \u2014 they're behavioral differences that catch people off guard.</p>"},{"location":"pain-points/#debugging-oom-out-of-memory-errors","title":"Debugging OOM (Out-of-Memory) errors","text":""},{"location":"pain-points/#how-oom-kills-look-in-slurm","title":"How OOM kills look in SLURM","text":"<p>When a job exceeds its memory allocation, SLURM kills it immediately. The job state is set to <code>OUT_OF_MEMORY</code>:</p> <pre><code>$ sacct -j 12345 --format=JobID,JobName,State,ExitCode,MaxRSS\nJobID           JobName      State ExitCode     MaxRSS\n------------ ---------- ---------- -------- ----------\n12345          analysis OUT_OF_ME+      0:125\n12345.batch       batch OUT_OF_ME+      0:125    15.8G\n</code></pre> <p>You can also see this with <code>seff</code>:</p> <pre><code>$ seff 12345\nJob ID: 12345\nState: OUT_OF_MEMORY (exit code 0)\nMemory Utilized: 15.80 GB\nMemory Efficiency: 98.75% of 16.00 GB\n</code></pre> <p>This is different from LSF</p> <p>On Bodhi's LSF, memory limits were often soft limits \u2014 jobs could exceed their requested memory without being killed (as long as the node had memory available). In SLURM, <code>--mem</code> is a hard limit enforced by cgroups. If your job exceeds it, even briefly, it will be killed.</p>"},{"location":"pain-points/#diagnosing-memory-usage","title":"Diagnosing memory usage","text":"<p>For completed jobs, use <code>sacct</code>:</p> <pre><code># Check peak memory usage\nsacct -j &lt;jobid&gt; --format=JobID,JobName,MaxRSS,MaxVMSize,State\n\n# For array jobs, check all tasks\nsacct -j &lt;jobid&gt; --format=JobID%20,JobName,MaxRSS,State\n</code></pre> <p>For running jobs, use <code>sstat</code>:</p> <pre><code># Monitor memory of a running job\nsstat -j &lt;jobid&gt; --format=JobID,MaxRSS,MaxVMSize\n</code></pre> <p>Use <code>seff</code> for quick checks</p> <p><code>seff &lt;jobid&gt;</code> gives a one-line summary of memory efficiency for completed jobs. It's the fastest way to check if your job was close to its memory limit.</p>"},{"location":"pain-points/#fixing-oom-errors","title":"Fixing OOM errors","text":"<ol> <li> <p>Check what your job actually used \u2014 run <code>seff &lt;jobid&gt;</code> on a similar completed job to see actual peak memory.</p> </li> <li> <p>Request more memory with headroom \u2014 add 20\u201330% buffer above the observed peak:</p> <pre><code>#SBATCH --mem=20G   # if your job peaked at ~15 GB\n</code></pre> </li> <li> <p>Use <code>--mem-per-cpu</code> for multi-threaded jobs \u2014 if your job scales memory with cores:</p> <pre><code>#SBATCH --cpus-per-task=8\n#SBATCH --mem-per-cpu=4G   # 32 GB total\n</code></pre> </li> </ol> <p>Don't just request the maximum</p> <p>Requesting far more memory than you need reduces scheduling priority and wastes cluster resources. Right-size your requests based on actual usage from <code>seff</code>.</p>"},{"location":"pain-points/#understanding-slurm-accounts","title":"Understanding SLURM accounts","text":""},{"location":"pain-points/#what-is-account","title":"What is <code>--account</code>?","text":"<p>In SLURM, the <code>--account</code> flag associates your job with a resource allocation account. This is used for:</p> <ul> <li>Fair-share scheduling \u2014 accounts that have used fewer resources recently get higher priority</li> <li>Resource tracking \u2014 PIs and admins can see how allocations are consumed</li> <li>Access control \u2014 some partitions may be restricted to certain accounts</li> </ul> <p>Why this matters on Bodhi</p> <p>On LSF, the <code>-P</code> project flag was often optional or had a simple default. On SLURM, submitting with the wrong account (or no account) can result in job rejection or lower scheduling priority.</p>"},{"location":"pain-points/#finding-your-accounts","title":"Finding your account(s)","text":"<pre><code># List your SLURM associations (accounts and partitions you can use)\nsacctmgr show associations user=$USER format=Account,Partition,QOS\n\n# Shorter version \u2014 just account names\nsacctmgr show associations user=$USER format=Account --noheader | sort -u\n</code></pre>"},{"location":"pain-points/#setting-a-default-account","title":"Setting a default account","text":"<p>Rather than adding <code>--account</code> to every script, set a default:</p> <pre><code># Set your default account (persists across sessions)\nsacctmgr modify user $USER set DefaultAccount=&lt;your_account&gt;\n</code></pre> <p>You can also add it to your <code>~/.bashrc</code> or a SLURM defaults file:</p> <pre><code># In ~/.bashrc\nexport SBATCH_ACCOUNT=&lt;your_account&gt;\nexport SRUN_ACCOUNT=&lt;your_account&gt;\n</code></pre> <p>Check your default</p> <pre><code>sacctmgr show user $USER format=DefaultAccount\n</code></pre>"},{"location":"pain-points/#in-your-job-scripts","title":"In your job scripts","text":"<pre><code>#SBATCH --account=&lt;your_account&gt;\n</code></pre>"},{"location":"pain-points/#paying-attention-to-wall-time","title":"Paying attention to wall time","text":""},{"location":"pain-points/#slurm-enforces-time-strictly","title":"SLURM enforces <code>--time</code> strictly","text":"<p>In SLURM, the <code>--time</code> (wall time) limit is a hard cutoff. When your job hits the limit:</p> <ol> <li>SLURM sends <code>SIGTERM</code> to your job (giving it a chance to clean up)</li> <li>After a short grace period, SLURM sends <code>SIGKILL</code></li> <li>The job state is set to <code>TIMEOUT</code></li> </ol> <pre><code>$ sacct -j 12345 --format=JobID,JobName,Elapsed,Timelimit,State\nJobID           JobName    Elapsed  Timelimit      State\n------------ ---------- ---------- ---------- ----------\n12345          longrun   02:00:00   02:00:00    TIMEOUT\n</code></pre> <p>This is different from LSF</p> <p>On Bodhi's LSF, wall-time limits were often loosely enforced \u2014 jobs could sometimes run past their <code>-W</code> limit. In SLURM, when your time is up, your job is killed. Period.</p>"},{"location":"pain-points/#checking-remaining-time","title":"Checking remaining time","text":"<p>From outside the job:</p> <pre><code># See time limit and elapsed time\nsqueue -u $USER -o \"%.10i %.20j %.10M %.10l %.6D %R\"\n#                              Elapsed ^  ^ Limit\n\n# Detailed view\nscontrol show job &lt;jobid&gt; | grep -E \"RunTime|TimeLimit\"\n</code></pre> <p>From inside the job (in your script):</p> <pre><code># Remaining time in seconds \u2014 useful for checkpointing\nsqueue -j $SLURM_JOB_ID -h -o \"%L\"\n</code></pre>"},{"location":"pain-points/#consequences-of-timeout","title":"Consequences of TIMEOUT","text":"<ul> <li>Your job output may be incomplete or corrupted</li> <li>Any files being written at kill time may be truncated</li> <li>Temporary files won't be cleaned up</li> </ul> <p>Add cleanup traps</p> <p>If your job writes large intermediate files, add a trap to handle <code>SIGTERM</code>:</p> <pre><code>cleanup() {\n    echo \"Job hit time limit \u2014 cleaning up\"\n    # save checkpoint, remove temp files, etc.\n}\ntrap cleanup SIGTERM\n</code></pre>"},{"location":"pain-points/#bodhi-partition-time-limits","title":"Bodhi partition time limits","text":"Partition Max wall time Default wall time Notes <code>short</code> 4 hours 1 hour Quick jobs, higher priority <code>normal</code> 7 days 1 hour General-purpose <code>long</code> 30 days 1 hour Extended runs <code>gpu</code> 7 days 1 hour GPU jobs <code>interactive</code> 12 hours 1 hour Interactive sessions <p>Check current limits</p> <p>Partition limits can change. Verify the current limits with:</p> <pre><code>sinfo -o \"%12P %10l %10L %6D %8c %10m\"\n#            Name  TimeLimit  DefTime  Nodes  CPUs  Memory\n</code></pre>"},{"location":"pain-points/#tips-for-setting-wall-time","title":"Tips for setting wall time","text":"<ol> <li> <p>Start with a generous estimate, then refine based on actual runtimes using <code>seff</code> or <code>sacct</code>.</p> </li> <li> <p>Shorter jobs schedule faster \u2014 SLURM's backfill scheduler can fit shorter jobs into gaps. Requesting 2 hours instead of 7 days can dramatically reduce queue wait time.</p> </li> <li> <p>Use <code>sacct</code> to check past runtimes:</p> <pre><code>sacct -u $USER --format=JobID,JobName,Elapsed,State -S 2024-01-01 | grep COMPLETED\n</code></pre> </li> <li> <p>SLURM format for <code>--time</code>:</p> Format Meaning <code>MM</code> Minutes <code>HH:MM:SS</code> Hours, minutes, seconds <code>D-HH:MM:SS</code> Days, hours, minutes, seconds <code>D-HH</code> Days and hours <pre><code>#SBATCH --time=04:00:00      # 4 hours\n#SBATCH --time=1-00:00:00    # 1 day\n#SBATCH --time=7-00:00:00    # 7 days\n</code></pre> </li> </ol>"},{"location":"resources/","title":"Resources","text":"<p>External references and guides for SLURM and LSF-to-SLURM migration.</p>"},{"location":"resources/#official-slurm-documentation","title":"Official SLURM documentation","text":"<ul> <li>SLURM Documentation \u2014 comprehensive reference for all SLURM commands and configuration</li> <li>sbatch Manual \u2014 all <code>#SBATCH</code> directives and <code>sbatch</code> options</li> <li>srun Manual \u2014 interactive and parallel job launch</li> <li>Job Array Support \u2014 detailed job array documentation</li> <li>sacct Manual \u2014 job accounting and history</li> </ul>"},{"location":"resources/#migration-guides","title":"Migration guides","text":"<ul> <li>SchedMD Rosetta Stone \u2014 the official PBS/LSF/SLURM command comparison (PDF)</li> <li>LLNL SLURM Tutorials \u2014 Lawrence Livermore National Laboratory's SLURM guides</li> <li>ETH Zurich LSF to SLURM \u2014 concise migration quick reference</li> <li>FIU LSF to SLURM \u2014 Florida International University's migration guide</li> </ul>"},{"location":"resources/#cheat-sheets","title":"Cheat sheets","text":"<ul> <li>SLURM Quick Reference (PDF) \u2014 two-page command summary</li> <li>SLURM Command Comparison \u2014 web version of the Rosetta Stone</li> </ul>"},{"location":"resources/#tools","title":"Tools","text":"<ul> <li>IBM lsf-slurm-wrappers \u2014 drop-in wrapper scripts that translate LSF commands to SLURM equivalents in real time</li> <li><code>lsf2slurm.sh</code> \u2014 this project's sed-based directive converter</li> </ul>"},{"location":"resources/#slurm-tips-for-lsf-users","title":"SLURM tips for LSF users","text":"<p>Key mindset shifts</p> <ol> <li>No input redirection: Use <code>sbatch script.sh</code>, not <code>sbatch &lt; script.sh</code></li> <li>Time format: SLURM prefers <code>HH:MM:SS</code> or <code>D-HH:MM:SS</code>; bare minutes are not standard</li> <li>Memory suffixes: Use <code>--mem=4G</code> instead of bare numbers to be explicit about units</li> <li><code>seff</code> for efficiency: After jobs complete, run <code>seff &lt;jobid&gt;</code> to see how well you utilized your allocation</li> <li><code>squeue</code> formatting: Customize with <code>--format</code> or set <code>SQUEUE_FORMAT</code> in your <code>.bashrc</code></li> <li>Default output: SLURM writes to <code>slurm-&lt;jobid&gt;.out</code> by default (LSF uses <code>LSFJOB_&lt;jobid&gt;/</code>)</li> </ol>"}]}